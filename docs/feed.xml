<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://hejohns.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hejohns.github.io/" rel="alternate" type="text/html" /><updated>2023-11-06T04:15:46-05:00</updated><id>https://hejohns.github.io/feed.xml</id><title type="html">hejohns’ jekyll site</title><subtitle>a pure waste of space</subtitle><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><entry><title type="html">Live Reload</title><link href="https://hejohns.github.io/site-construction/2023/11/06/live-reload.html" rel="alternate" type="text/html" title="Live Reload" /><published>2023-11-06T00:00:00-05:00</published><updated>2023-11-06T04:10:10-05:00</updated><id>https://hejohns.github.io/site-construction/2023/11/06/live-reload</id><content type="html" xml:base="https://hejohns.github.io/site-construction/2023/11/06/live-reload.html"><![CDATA[<p>I’ve been writing posts without live reloading/preview
for some reason.
I think <code class="language-plaintext highlighter-rouge">--watch</code> didn’t work or something.</p>

<p>In any case, I just tried this while editing the homepage,
and it’s so nice, I had to make this post.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundler exec jekyll serve -l --lsi -H 192.168.1.100
</code></pre></div></div>

<p>Sometimes I have to manually refresh the browser page though
or else it gets stuck on a blank screen.</p>

<p>(And of course, check out the other <a href="/category/site-construction/">site-construction</a> posts)</p>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><category term="site-construction" /><summary type="html"><![CDATA[I’ve been writing posts without live reloading/preview for some reason. I think --watch didn’t work or something.]]></summary></entry><entry><title type="html">coordinating processes</title><link href="https://hejohns.github.io/2023/10/29/coordinating-processes.html" rel="alternate" type="text/html" title="coordinating processes" /><published>2023-10-29T00:00:00-04:00</published><updated>2023-10-30T06:23:51-04:00</updated><id>https://hejohns.github.io/2023/10/29/coordinating-processes</id><content type="html" xml:base="https://hejohns.github.io/2023/10/29/coordinating-processes.html"><![CDATA[<p>This has to be a solved problem?!</p>

<p><a href="https://github.com/hejohns/systemp">repo</a></p>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[This has to be a solved problem?!]]></summary></entry><entry><title type="html">tortoise text to speech (and piper)</title><link href="https://hejohns.github.io/2023/10/10/tortoise-tts.html" rel="alternate" type="text/html" title="tortoise text to speech (and piper)" /><published>2023-10-10T00:00:00-04:00</published><updated>2023-10-11T06:33:16-04:00</updated><id>https://hejohns.github.io/2023/10/10/tortoise-tts</id><content type="html" xml:base="https://hejohns.github.io/2023/10/10/tortoise-tts.html"><![CDATA[<h2 id="motivation">Motivation</h2>

<p>I’m getting my butt kicked by EECS 582.
We’re assigned four papers a week to read–
two for Tuesday, two for Thursday–
and it’s just too much to read with attention to detail.
But I don’t think the point <em>is</em> to read every paper in detail.
Or at least that’s not what I really want out of it.</p>

<p>This seems to me an exercise in how to quickly synthesize/accumulate/index
ideas and knowledge,
and how to summarize/distill it quickly.
(The paper reviews used to take me hours, but there just isn’t the time for that.
 I’ve been cramming it into ~45 minutes.)</p>

<p>As such, I’m looking for fast ways to get to the summary,
if that’s the end-goal, so to speak.
There’s some tips and tricks people will tell you when it comes to reading papers,
but I wanted to try something more aggressive.</p>

<h2 id="text-to-speech-tools">text to speech tools</h2>

<p>My eyes were feeling uncomfortable from (I have no medical backing to say this)
all the, well, load, of screen + reading time,
so the natural conclusion was to try <em>listening</em> to papers.</p>

<p>So I copy/pasted the entire paper pdf into a text file,
removed line breaks (eg “libr-\nary”),
and converted the text to audio with a tts tool.</p>

<p>Of course this requires that the paper has signifiant amounts of prose.
The papers we’re reading in 582 do<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> (and more on the effectiveness of the tts tools on papers below), but I have yet to try other areas.</p>

<p>I first tried some online tools, but wasn’t very satisfied.
I’m sure there are good services online though.</p>

<p>In any case, I ended up trying local text to speech tools,
and the two that I got working were:</p>
<ul>
  <li><a href="https://github.com/rhasspy/piper">piper</a></li>
  <li><a href="https://github.com/neonbjb/tortoise-tts">tortoise-tts</a></li>
</ul>

<p>(festival kept crashing, and I think I forgot to try eSpeak-ng.)</p>

<h3 id="piper">piper</h3>
<p>There’s not much to say.
Piper “just works”.
It’s fast– it can stream audio– and sounds good enough to listen to papers.
I’ve been using **cpu inference. I’m not sure if it supports gpu.</p>

<p>This is what I’ve actually been using since tortoise…
didn’t “just work” out of the box.</p>

<p>For some reason, the wav file piper produces isn’t readable on my sony icd,
so I had to ffmpeg it to an mp3.</p>
<h3 id="tortoise">tortoise</h3>
<p>Okay… this is the real reason I wrote this post.</p>

<p>The output quality of tortoise is fantastic.
Yes, it’s slow, but I’m willing to put up with it if it works–
just run it overnight/overnight + a day.
I have yet to really evaluate it as a paper reading tool, because I’ve been so bogged down trying to get it working.</p>

<h4 id="docker">docker</h4>
<p>First, <strong>cpu</strong> only:</p>

<p>The Dockerfile provided works with some small modifications
which others noted in the issue comments and a PR.
So I made a <a href="https://github.com/eecs582-andlars-hasinha-hejohns/tortoise-tts">fork</a>
with those patches.</p>

<p>tortoise hardcodes cuda in some places, so a couple patches are needed, but it does run with only cpu.
Just invoke <code class="language-plaintext highlighter-rouge">docker run</code> without <code class="language-plaintext highlighter-rouge">--gpus all</code>.</p>
<h4 id="gpu">gpu</h4>
<p>I have an nvidia mobile gpu on my desktop (the one from <a href="/2023/05/10/linux-graphics-modules.html">the post about nvidia kernel modules</a>,
so I thought I’d try to see how it compares to cpu tortoise,
which I’d been running on a dual xeon gold 6142.</p>

<p>It became issue after issue.</p>

<p>So first you have to install <code class="language-plaintext highlighter-rouge">nvidia-container-toolkit</code>.
Then when I added <code class="language-plaintext highlighter-rouge">--gpus all</code> back to tortoise’s <code class="language-plaintext highlighter-rouge">docker run</code>,
docker failed to pickup any gpus.</p>

<p>Okay… everything seemed fine with the gpu setup (<code class="language-plaintext highlighter-rouge">nvidia-smi</code> shows output, no systemd nvidia service errors like <a href="/2023/05/10/linux-graphics-modules.html">the post about nvidia kernel modules</a>),
except <code class="language-plaintext highlighter-rouge">nvidia-detect</code> errored out,
complaining about not being able to identify the debian version.</p>

<p>After a display of poor debugging skills, I finally looked at <code class="language-plaintext highlighter-rouge">vim $(which nvidia-detect)</code> and searched for the error message.
It turned out the nvidia-detect script just hardcodes some logic for each debian release,
and looks up the value in <code class="language-plaintext highlighter-rouge">/etc/debian_version</code> to dispatch one of the cases.
If <code class="language-plaintext highlighter-rouge">/etc/debian_version</code> doesn’t match any of the cases,
an error message prints about not being able to identify  the debian version.</p>

<p>Well, I have debian testing on my desktop, with
<code class="language-plaintext highlighter-rouge">cat /etc/debian_version</code><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> showing <code class="language-plaintext highlighter-rouge">trixie/sid</code>.
Which just wasn’t a case.
nvidia-detect only hardcoded up to bookworm/debian 12.</p>

<p>My best guess is that the proper fix is to just wait for a newer nvidia-detect.
But I wanted some more immediate closure, so I manually set <code class="language-plaintext highlighter-rouge">/etc/debian_version</code>
to “bookworm/12”, and everyone was happy.</p>

<p>Then docker <code class="language-plaintext highlighter-rouge">--gpus all</code> was finally happy, but tortoise still wasn’t using the gpu.
Another <em>insert search engine</em> adventure later about cuda versions
suggested that I did in fact have a card that supported cuda 12.0,
which should work with tortoise
(I think tortoise itself wanted cuda 11.7, and the Dockerfile wanted the nvidia cuda 12.2 docker image, which may have worked, but I downgraded it to 12.0 to match my host system).</p>

<p>Then the online oracles said to run <code class="language-plaintext highlighter-rouge">python -m torch.utils.collect_env</code>
to see what pytorch was up to.
And indeed, it showed that pytorch was <em>not</em> built with cuda.</p>

<p>I’m not sure why, given that the Dockerfile seems to install pytorch with cuda,
but in any case, there’s no shortage of hacks here, so the oracle said to
just <code class="language-plaintext highlighter-rouge">pip uninstall torch</code> and reinstall it with cuda, as per <a href="https://pytorch.org/get-started/locally/">the pytorch site</a>,
so I did,
and <code class="language-plaintext highlighter-rouge">python -m torch.utils.collect_env</code> changed to show that cuda was enabled.
This is all in a docker container, so I don’t actually mind how much of a hack this is.</p>

<p>With pytorch finally gpu enabled, I fired up tortoise for the nth time,
and it quickly died with a cuda out of memory error.</p>

<p>The cognoscenti may have noticed that I wanted to do this on a quadro M620
with 2GB ram.
Me being an ML-ignorant fool, didn’t think that that was insufficient.</p>

<p>Even with <code class="language-plaintext highlighter-rouge">batch_size = 1</code>, as small as possible<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>,
tortoise still kept running out of memory.
From what I could find in the issue comments, 4GB is considered a small memory footprint, so it may be that tortoise simply cannot run on 2GB gpu ram.</p>

<p>So I finally got some closure, and four hours less sleep,
but on the plus side,
docker’s quite pleasant to use?</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>figures don’t meaningfully work if they do at all. They usually don’t get copied as text, and if some parts of the figure do, it’s all mangled. That said, captions <em>do</em> work, and are sometimes enough to get the general idea. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>what is this used for?? <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>I’m assuming that means something to the ML-literate <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[Motivation]]></summary></entry><entry><title type="html">nic bonding</title><link href="https://hejohns.github.io/2023/08/03/nic-bonding.html" rel="alternate" type="text/html" title="nic bonding" /><published>2023-08-03T00:00:00-04:00</published><updated>2023-08-03T17:41:06-04:00</updated><id>https://hejohns.github.io/2023/08/03/nic-bonding</id><content type="html" xml:base="https://hejohns.github.io/2023/08/03/nic-bonding.html"><![CDATA[<ul>
  <li>nmtui</li>
  <li>edit a connection</li>
  <li>add</li>
  <li>type bond</li>
  <li>two slaves
    <ul>
      <li>idk why but it seems like you need to create two entirely new connections</li>
      <li>so deactivate the old connections before activating the bond</li>
    </ul>
  </li>
  <li>set primary device (eg <code class="language-plaintext highlighter-rouge">eno2</code>). This is the mac address used?</li>
  <li>status:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">nmcli con</code></li>
      <li><code class="language-plaintext highlighter-rouge">ip address</code></li>
    </ul>
  </li>
  <li>the mac address changes (not sure why), so you probably have to change the dhcp server settings to statically lease the old ip to the bond mac address</li>
</ul>

<p>redhat has some documentation online</p>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[nmtui edit a connection add type bond two slaves idk why but it seems like you need to create two entirely new connections so deactivate the old connections before activating the bond set primary device (eg eno2). This is the mac address used? status: nmcli con ip address the mac address changes (not sure why), so you probably have to change the dhcp server settings to statically lease the old ip to the bond mac address]]></summary></entry><entry><title type="html">book catalogue</title><link href="https://hejohns.github.io/2023/07/24/book-catalogue.html" rel="alternate" type="text/html" title="book catalogue" /><published>2023-07-24T00:00:00-04:00</published><updated>2023-09-30T14:54:48-04:00</updated><id>https://hejohns.github.io/2023/07/24/book-catalogue</id><content type="html" xml:base="https://hejohns.github.io/2023/07/24/book-catalogue.html"><![CDATA[<ul id="markdown-toc">
  <li><a href="#logic" id="markdown-toc-logic">logic</a>    <ul>
      <li><a href="#introductory-textbooks" id="markdown-toc-introductory-textbooks">introductory textbooks</a></li>
      <li><a href="#monographs" id="markdown-toc-monographs">monographs</a></li>
      <li><a href="#editorials-and-proceedings" id="markdown-toc-editorials-and-proceedings">editorials and proceedings</a></li>
      <li><a href="#misc" id="markdown-toc-misc">misc</a></li>
    </ul>
  </li>
  <li><a href="#computer-science" id="markdown-toc-computer-science">computer science</a>    <ul>
      <li><a href="#programming-languages" id="markdown-toc-programming-languages">programming languages</a></li>
      <li><a href="#other" id="markdown-toc-other">other</a></li>
    </ul>
  </li>
  <li><a href="#novels-and-related" id="markdown-toc-novels-and-related">novels and related</a></li>
  <li><a href="#wishlist" id="markdown-toc-wishlist">wishlist</a></li>
</ul>

<p>This evening, I made my largest single import of books to date.
A certain logician is moving to another university– upwards in the academic food chain–
and offered up some of their books for grabs.
I thought I should catalogue the lot, both for my own sake, and as a way to increase their utilization.
Personal libraries are great, but something is quite sad when you think about a book’s realized reading time v. how often it just sits on the shelf.
Or in my case, a box.</p>

<p>Extending this to my current collection, the below is a best effort catalogue of what I have.
Any listing is loanable– just ask.</p>

<p>Each section is roughly in decreasing order of rank, which is an opaque combination of whether I’ve read it, enjoyment, perceived reputation/reverence/significance, etc.</p>

<h2 id="logic">logic</h2>
<h3 id="introductory-textbooks">introductory textbooks</h3>
<ul>
  <li>set theory?</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Shoenfield</td>
          <td>Mathematical Logic</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Chang, Keisler</td>
          <td>Model Theory</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Mendelson</td>
          <td>Introduction to Mathematical Logic</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Hinman</td>
          <td>Fundamentals of Mathematical Logic</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Avigad</td>
          <td>Mathematical Logic and Computation</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Takeuti</td>
          <td>Proof Theory</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>phil?
    <h3 id="monographs">monographs</h3>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Kleene, Vesley</td>
          <td>The Foundation of Intuitionistic Mathematics</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Hinman</td>
          <td>Recursion-Theoretic Hierarchies</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Buss | Bounded Arithmetic
    <h3 id="editorials-and-proceedings">editorials and proceedings</h3>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>van Heijenoort</td>
          <td>From Frege to Gödel</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Heyting</td>
          <td>Constructivity in Mathematics</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Proceedings of the Herbrand Symposium Logic Colloquium ‘81
    <h3 id="misc">misc</h3>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Menzler-Trott</td>
          <td>Logic’s Lost Genius: The life of Gerhard Gentzen</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<h2 id="computer-science">computer science</h2>
<h3 id="programming-languages">programming languages</h3>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Harper</td>
          <td>Practical Foundations for Programming Languages</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Pierce | Advanced Types and Programming Languages
    <h3 id="other">other</h3>
  </li>
  <li>OSTEP</li>
</ul>

<h2 id="novels-and-related">novels and related</h2>
<ul>
  <li>tanizaki</li>
  <li>childhood years</li>
  <li>dazai</li>
</ul>

<h2 id="wishlist">wishlist</h2>
<p>ie books that I’ve read small bits of and are just so so good</p>

<ul>
  <li>Soare’s computability theory book</li>
  <li>Marker’s model theory book</li>
</ul>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author></entry><entry><title type="html">equivalence of categories</title><link href="https://hejohns.github.io/category-theory/2023/05/18/equivalence-of-categories.html" rel="alternate" type="text/html" title="equivalence of categories" /><published>2023-05-18T00:00:00-04:00</published><updated>2023-05-18T21:51:35-04:00</updated><id>https://hejohns.github.io/category-theory/2023/05/18/equivalence-of-categories</id><content type="html" xml:base="https://hejohns.github.io/category-theory/2023/05/18/equivalence-of-categories.html"><![CDATA[<p>An oft-stated characterization of an equivalence of categories is that TFAE:</p>
<ol>
  <li>A functor \(F : \mathcal{C} → \mathcal{D}\) is (the forward direction of) an equivalence of categories</li>
  <li>\(F\) is <em>full</em>, <em>faithful</em>, and <em>essentially surjective on objects</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></li>
</ol>

<p><strong>pf (1 ⟹ 2):</strong> (you really need to draw the diagrams. I’m not going to try here)</p>

<p>Let \(F : \mathcal{C} → \mathcal{D}\) be an equivalence of categories.</p>
<ul>
  <li>wts \(F\) faithful: assume \(Ff = Fg\), for \(f, g : \mathcal{C}(A, B)\).
Then \(F⁻¹Ff = F⁻¹Fg\).
Let \(α : 1_\mathcal{C} ⇒ F⁻¹F\) witness that F is an equivalence.
Notice that \(α_B ∘ f = F⁻¹Ff ∘ α_A ⟹ α_B ∘ f ∘ α⁻¹_A = F⁻¹Ff\),
and similarly \(α_B ∘ g = F⁻¹Fg ∘ α_A ⟹ α_B ∘ g ∘ α⁻¹_A = F⁻¹Fg\).
Thus \(F⁻¹Ff = F⁻¹Fg ⟹ F⁻¹Fg ∘ α_A ⟹ α_B ∘ f ∘ α⁻¹_A = F⁻¹Fg ∘ α_A ⟹ α_B ∘ g ∘ α⁻¹_A ⟹ f = g\).<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
</ul>

<ul>
  <li>wts \(F\) full: Consider \(h : \mathcal{D}(FA, FB)\).
Then \(α⁻¹_B ∘ F⁻¹h ∘ α_A : \mathcal{C}(A, B)\)
In a roundabout fashion, consider not \(F(α⁻¹_B ∘ F⁻¹h ∘ α_A)\) directly,
but \(F⁻¹F(α⁻¹_B ∘ F⁻¹h ∘ α_A)\).
Actually, our proof above that \(F\) is faithful is symmetric since an equivalence of categories is a symmetric notion, so \(F⁻¹\) is faithful as well.
Notice that \(F⁻¹F(α⁻¹_B ∘ F⁻¹h ∘ α_A) = α_B ∘ f ∘ α⁻¹_A = F⁻¹h\),
and since \(F⁻¹\) is faithful, \(F(α⁻¹_B ∘ F⁻¹h ∘ α_A) = h\).</li>
  <li>wts \(F\) eso:
Let \(β : FF⁻¹ ⇒ 1_\mathcal{D}\) be the other witness that \(\mathcal{C} ≅ \mathcal{D}\).
\(∀X ∈ \mathcal{D}. β_X\) witnesses that \(FF⁻¹X ≅ X\).</li>
</ul>

<p><strong>pf (2 ⟹ 1):</strong></p>

<p>Let \(F\) be fully faithful and eso.
Define \(F⁻¹\) st</p>

<p>TODO: I’m tired. This is Awodey p.174</p>

<p>Amalgamated from Awodey and Rhiel’s books, and the nlab</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>the definitions are standard, but the way I remember it is that <em>faithful</em> := injective on homsets, <em>full</em> := surjective on homsets, <em>eso</em> := surjective on objects, up to isomorphism <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>we make use of the fact that natural transformation ⟹ (natural isomorphism ⟺ each point is an isomorphism) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><category term="category-theory" /><summary type="html"><![CDATA[An oft-stated characterization of an equivalence of categories is that TFAE: A functor \(F : \mathcal{C} → \mathcal{D}\) is (the forward direction of) an equivalence of categories \(F\) is full, faithful, and essentially surjective on objects1 the definitions are standard, but the way I remember it is that faithful := injective on homsets, full := surjective on homsets, eso := surjective on objects, up to isomorphism &#8617;]]></summary></entry><entry><title type="html">moving from git to darcs</title><link href="https://hejohns.github.io/site-construction/2023/05/14/darcs.html" rel="alternate" type="text/html" title="moving from git to darcs" /><published>2023-05-14T00:00:00-04:00</published><updated>2023-07-05T20:26:13-04:00</updated><id>https://hejohns.github.io/site-construction/2023/05/14/darcs</id><content type="html" xml:base="https://hejohns.github.io/site-construction/2023/05/14/darcs.html"><![CDATA[<p>I played around with <code class="language-plaintext highlighter-rouge">darcs</code> two summers ago,
but in this day and age everyone uses <code class="language-plaintext highlighter-rouge">git</code> so it’s just not practical when working with other people.</p>

<p>That said, this is my personal site and I can do whatever I want, so I’m firing darcs back up!</p>

<p>The darcs site <a href="http://darcs.net/Using/Convert">has a page</a> for converting between git and darcs and forth,
although I couldn’t get the mark files to work,
and when exporting from darcs to git,
I needed to <code class="language-plaintext highlighter-rouge">sed</code> (or more precisely  <code class="language-plaintext highlighter-rouge">darcs convert export | perl -pe 's#refs/heads/trunk#refs/heads/trunk#g' | (cd ../hejohns.github.io_git-mirror/ &amp;&amp; git fast-import)</code>)
to import into the correct git branch.</p>

<p><strong>NOTE</strong>: Should be “refs/heads/<strong>master</strong>#refs/heads/trunk”. I don’t know why but it keeps showing up as trunk twice when I export</p>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><category term="site-construction" /><summary type="html"><![CDATA[I played around with darcs two summers ago, but in this day and age everyone uses git so it’s just not practical when working with other people.]]></summary></entry><entry><title type="html">multiple displays and linux graphics modules</title><link href="https://hejohns.github.io/2023/05/10/linux-graphics-modules.html" rel="alternate" type="text/html" title="multiple displays and linux graphics modules" /><published>2023-05-10T00:00:00-04:00</published><updated>2023-05-15T03:04:57-04:00</updated><id>https://hejohns.github.io/2023/05/10/linux-graphics-modules</id><content type="html" xml:base="https://hejohns.github.io/2023/05/10/linux-graphics-modules.html"><![CDATA[<p>So I got a HP z2 mini g3 recently
(to replace an existing dual monitor setup, which is relevant.
The optiplex 780 usff and its core 2 duo E8400 is a real workhorse.),
which has an intel xeon e3-1225 v6
(which could actually replace the pentium G4600 in my T130…<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>)
and a mobile nvidia quadro m620.</p>

<p>The box has four display port ports in the back, along with a BIOS option to send the integrated graphics along one of the ports.
One monitor worked fine. I only had a single display port adaptor<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">2</a></sup>, so I had to wait until today to hook up the second monitor.</p>

<p>But I ran into this strange issue where only one display port would work at a time, integrated graphics disabled,
and when I enabled the integrated graphics to try to run two monitors 1-1, X11 completely failed to start for reasons mere mortals like myself can’t begin to comprehend.
Apparently the red flag should of been the tons of <code class="language-plaintext highlighter-rouge">modprobe</code> ‘key rejected’ or ‘drm’ errors and the like,
and that <code class="language-plaintext highlighter-rouge">systemctl status --failed</code> complained about <code class="language-plaintext highlighter-rouge">nvidia-persistenced</code> and whatever the kernel module loader service is<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">3</a></sup>.</p>

<p>There’s probably a downside to this I should care about but it seems like <em>disabling secure boot</em> does the trick and <code class="language-plaintext highlighter-rouge">systemctl status --failed</code> no longer shows anything.</p>

<p>I actually didn’t try this until I tried updating the BIOS, and one of the steps– which required loading a kernel module– failed due to the signing issue.
I don’t understand security so I’m going w/ the wrong answer for now and just disabling all that stuff.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>especially while the disposition center has the z2 mini g3 ‘s for less than what you typically find just the e3-1225 v6 for… <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>my monitors are old. VGA always works; I guess DVI-D is alright. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I mean… the single display worked fine with all the errors? <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[So I got a HP z2 mini g3 recently (to replace an existing dual monitor setup, which is relevant. The optiplex 780 usff and its core 2 duo E8400 is a real workhorse.), which has an intel xeon e3-1225 v6 (which could actually replace the pentium G4600 in my T130…1) and a mobile nvidia quadro m620. especially while the disposition center has the z2 mini g3 ‘s for less than what you typically find just the e3-1225 v6 for… &#8617;]]></summary></entry><entry><title type="html">shelving studying nonstandard analysis</title><link href="https://hejohns.github.io/2023/05/07/nonstandard-analysis.html" rel="alternate" type="text/html" title="shelving studying nonstandard analysis" /><published>2023-05-07T00:00:00-04:00</published><updated>2023-05-15T03:04:57-04:00</updated><id>https://hejohns.github.io/2023/05/07/nonstandard-analysis</id><content type="html" xml:base="https://hejohns.github.io/2023/05/07/nonstandard-analysis.html"><![CDATA[<p>(personal record keeping)</p>

<p>I’ve whiled away my chances last semester to take a close look at nonstandard analysis while I had the time,
and now I can no longer set aside focus for it in good conscience.
With pressing matters of higher priority, I formally declare, with great regret,
an indefinite hiatus of this project, marked by the returning of Goldblatt’s book.</p>

<p>It was an excellent time to study nonstandard analysis,
on the tails of the excellent model theory class and continuous logic seminar,
but the model theoretic background seems low enough that picking Goldblatt’s book back up should be smooth.</p>

<p>Alas. One day. One day.</p>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[(personal record keeping)]]></summary></entry><entry><title type="html">chili oil</title><link href="https://hejohns.github.io/2023/05/02/chili-oil.html" rel="alternate" type="text/html" title="chili oil" /><published>2023-05-02T00:00:00-04:00</published><updated>2023-05-15T03:04:57-04:00</updated><id>https://hejohns.github.io/2023/05/02/chili-oil</id><content type="html" xml:base="https://hejohns.github.io/2023/05/02/chili-oil.html"><![CDATA[<p>since I always manage to forget something so simple:</p>
<ul>
  <li>take the thai chili or equivalent</li>
  <li>remove stem, and remove tip as it’s prone to burning</li>
  <li>the seeds burn slower than the flesh, so</li>
  <li>cut in half lengthwise</li>
  <li>remove the seeds and put them in a spoon</li>
  <li>cut the flesh into quarters or some sort of strip
    <ul>
      <li>if the pieces are too small, they tend to burn</li>
      <li>if the pieces are too large, it’s a waste of pepper</li>
    </ul>
  </li>
  <li>heat neutral oil in the smallest saucepan</li>
  <li>seeds in first, dump the spoon</li>
  <li>wait a bit, then fresh in</li>
  <li>high heat quickly, then immediately off to prevent burning</li>
</ul>]]></content><author><name>Johnson He</name><email>(hejohns)α(umich)δ(edu)</email></author><summary type="html"><![CDATA[since I always manage to forget something so simple: take the thai chili or equivalent remove stem, and remove tip as it’s prone to burning the seeds burn slower than the flesh, so cut in half lengthwise remove the seeds and put them in a spoon cut the flesh into quarters or some sort of strip if the pieces are too small, they tend to burn if the pieces are too large, it’s a waste of pepper heat neutral oil in the smallest saucepan seeds in first, dump the spoon wait a bit, then fresh in high heat quickly, then immediately off to prevent burning]]></summary></entry></feed>
